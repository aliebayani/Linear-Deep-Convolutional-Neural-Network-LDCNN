{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXaoYzaeH1KxjiqO59A0lj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV:**"
      ],
      "metadata": {
        "id": "CMqoy18yd52J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "wSGaZ0Q4awK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5ext157adfgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import pywt\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, AvgPool1D, Flatten, Dense, Dropout, Softmax\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.activations import relu, sigmoid\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "V0jKkVvfefaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (30,6)\n",
        "plt.rcParams['lines.linewidth'] = 1\n",
        "plt.rcParams['lines.color'] = 'b'\n",
        "plt.rcParams['axes.grid'] = True"
      ],
      "metadata": {
        "id": "8gbGc4nKeihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denoise(data):\n",
        "    w = pywt.Wavelet('sym4')\n",
        "    maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
        "    threshold = 0.04 # Threshold for filtering\n",
        "\n",
        "    coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n",
        "\n",
        "    datarec = pywt.waverec(coeffs, 'sym4')\n",
        "\n",
        "    return datarec"
      ],
      "metadata": {
        "id": "umOQcPEaekSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/data_mit/'\n",
        "window_size = 180\n",
        "maximum_counting = 10000\n",
        "\n",
        "classes = ['N', 'L', 'R', 'A', 'V']\n",
        "n_classes = len(classes)\n",
        "count_classes = [0]*n_classes\n",
        "\n",
        "X = list()\n",
        "y = list()"
      ],
      "metadata": {
        "id": "auCiF9t2el6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = next(os.walk(path))[2]\n",
        "\n",
        "# Split and save .csv , .txt\n",
        "records = list()\n",
        "annotations = list()\n",
        "filenames.sort()"
      ],
      "metadata": {
        "id": "KTz0XxS6ennx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in filenames:\n",
        "    filename, file_extension = os.path.splitext(f)\n",
        "\n",
        "    # *.csv\n",
        "    if(file_extension == '.csv'):\n",
        "        records.append(path + filename + file_extension)\n",
        "\n",
        "    # *.txt\n",
        "    else:\n",
        "        annotations.append(path + filename + file_extension)"
      ],
      "metadata": {
        "id": "5_7oXvAweq_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "for r in range(0,len(records)):\n",
        "    signals = []\n",
        "\n",
        "    with open(records[r], 'rt') as csvfile:\n",
        "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\n",
        "        row_index = -1\n",
        "        for row in spamreader:\n",
        "            if(row_index >= 0):\n",
        "                signals.insert(row_index, int(row[1]))\n",
        "            row_index += 1\n",
        "\n",
        "    # Plot an example to the signals\n",
        "    if r is 1:\n",
        "        # Plot each patient's signal\n",
        "        plt.title(\"Wave\", fontsize=20)\n",
        "        plt.plot(signals[0:700], color='darkred')\n",
        "        plt.show()\n",
        "\n",
        "    signals = denoise(signals)\n",
        "    # Plot an example to the signals\n",
        "    if r is 1:\n",
        "        # Plot each patient's signal\n",
        "        plt.title(\"Wave after denoised\", fontsize=20)\n",
        "        plt.plot(signals[0:700], color='darkred')\n",
        "        plt.show()\n",
        "\n",
        "    signals = stats.zscore(signals)\n",
        "    # Plot an example to the signals\n",
        "    if r is 1:\n",
        "        # Plot each patient's signal\n",
        "        plt.title(\"Wave after z-score normalization\", fontsize=20)\n",
        "        plt.plot(signals[0:700], color='darkred')\n",
        "        plt.show()\n",
        "\n",
        "    # Read anotations: R position and Arrhythmia class\n",
        "    example_beat_printed = False\n",
        "    with open(annotations[r], 'r') as fileID:\n",
        "        data = fileID.readlines()\n",
        "        beat = list()\n",
        "\n",
        "        for d in range(1, len(data)): # 0 index is Chart Head\n",
        "            splitted = data[d].split(' ')\n",
        "            splitted = filter(None, splitted)\n",
        "            next(splitted) # Time... Clipping\n",
        "            pos = int(next(splitted)) # Sample ID\n",
        "            arrhythmia_type = next(splitted) # Type\n",
        "            if(arrhythmia_type in classes):\n",
        "                arrhythmia_index = classes.index(arrhythmia_type)\n",
        "#                 if count_classes[arrhythmia_index] > maximum_counting: # avoid overfitting\n",
        "#                     pass\n",
        "#                 else:\n",
        "                count_classes[arrhythmia_index] += 1\n",
        "                if(window_size <= pos and pos < (len(signals) - window_size)):\n",
        "                    beat = signals[pos-window_size:pos+window_size]     # REPLACE WITH R-PEAK DETECTION\n",
        "                    # Plot an example to a beat\n",
        "                    if r is 1 and not example_beat_printed:\n",
        "                        plt.title(\"A Beat from this wave\", fontsize=20)\n",
        "                        plt.plot(beat, color='darkred')\n",
        "                        plt.show()\n",
        "                        example_beat_printed = True\n",
        "\n",
        "                    X.append(beat)\n",
        "                    y.append(arrhythmia_index)\n",
        "\n",
        "# data shape\n",
        "print(np.shape(X), np.shape(y))"
      ],
      "metadata": {
        "id": "kqK0TQLBevqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(X)):\n",
        "        X[i] = np.append(X[i], y[i])\n",
        "\n",
        "print(np.shape(X))"
      ],
      "metadata": {
        "id": "w-AzAw_seytU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df = pd.DataFrame(X)\n",
        "per_class = X_train_df[X_train_df.shape[1]-1].value_counts()\n",
        "print(per_class)\n"
      ],
      "metadata": {
        "id": "JAJHyR0ce0zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A method called **resampling**, which adjusts the number of majority and minority instances, is usually used to **solve the imbalance in training data**. Although resampling can **eliminate imbalances**, it may cause data complexity that deteriorates classification accuracy:"
      ],
      "metadata": {
        "id": "B-7sipxXYlUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1=X_train_df[X_train_df[X_train_df.shape[1]-1]==1]\n",
        "df_2=X_train_df[X_train_df[X_train_df.shape[1]-1]==2]\n",
        "df_3=X_train_df[X_train_df[X_train_df.shape[1]-1]==3]\n",
        "df_4=X_train_df[X_train_df[X_train_df.shape[1]-1]==4]\n",
        "# df_5=X_train_df[X_train_df[X_train_df.shape[1]-1]==5]\n",
        "df_0=(X_train_df[X_train_df[X_train_df.shape[1]-1]==0]).sample(n=5000,random_state=42)\n",
        "\n",
        "df_1_upsample=resample(df_1,replace=True,n_samples=5000,random_state=122)\n",
        "df_2_upsample=resample(df_2,replace=True,n_samples=5000,random_state=123)\n",
        "df_3_upsample=resample(df_3,replace=True,n_samples=5000,random_state=124)\n",
        "df_4_upsample=resample(df_4,replace=True,n_samples=5000,random_state=125)\n",
        "# df_5_upsample=resample(df_5,replace=True,n_samples=5000,random_state=126)\n",
        "\n",
        "# X_train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample,df_5_upsample])\n",
        "X_train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])"
      ],
      "metadata": {
        "id": "weqdYrYXe25O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_class = X_train_df[X_train_df.shape[1]-1].value_counts()\n",
        "print(per_class)"
      ],
      "metadata": {
        "id": "T8ej2wb5e4jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(X_train_df, test_size=0.20)\n",
        "\n",
        "print(\"X_train : \", np.shape(train))\n",
        "print(\"X_test  : \", np.shape(test))"
      ],
      "metadata": {
        "id": "plxepauAe6i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train=train[train.shape[1]-1]\n",
        "target_test=test[test.shape[1]-1]\n",
        "train_y=to_categorical(target_train)\n",
        "test_y=to_categorical(target_test)\n",
        "print(np.shape(train_y), np.shape(test_y))"
      ],
      "metadata": {
        "id": "OgZQY9k5fE74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train.iloc[:,:train.shape[1]-1].values\n",
        "test_x = test.iloc[:,:test.shape[1]-1].values\n",
        "train_x = train_x.reshape(len(train_x), train_x.shape[1],1)\n",
        "test_x = test_x.reshape(len(test_x), test_x.shape[1],1)\n",
        "print(np.shape(train_x), np.shape(test_x))"
      ],
      "metadata": {
        "id": "7Z9kfrrxfG2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9KoNouwmWK3"
      },
      "outputs": [],
      "source": [
        "# Define a function that builds your model\n",
        "def create_model(learning_rate=0.001, epochs=10, optimizer='adam', batch_size=36, filters=16, kernel_size=13, dropout=0.5, activation='relu', l2_regularization=0.0001, pool_size=2, strides=1):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation=activation, input_shape=(360, 1)))\n",
        "    model.add(AvgPool1D(pool_size=pool_size, strides=2))\n",
        "    model.add(Conv1D(filters=filters*2, kernel_size=kernel_size+2, padding='same', activation=activation))\n",
        "    model.add(AvgPool1D(pool_size=pool_size, strides=2))\n",
        "    model.add(Conv1D(filters=filters*4, kernel_size=kernel_size+4, padding='same', activation=activation))\n",
        "    model.add(AvgPool1D(pool_size=pool_size, strides=2))\n",
        "    model.add(Conv1D(filters=filters*8, kernel_size=kernel_size+6, padding='same', activation=activation))\n",
        "    model.add(AvgPool1D(pool_size=pool_size, strides=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(35, kernel_regularizer=l2(l2_regularization), bias_regularizer=l2(l2_regularization), activation=activation))\n",
        "    model.add(Dense(5, kernel_regularizer=l2(l2_regularization), bias_regularizer=l2(l2_regularization), activation=activation))\n",
        "    model.add(Softmax())\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Wrap your Keras model with the KerasClassifier wrapper:\n",
        "keras_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Define the hyperparameters and their possible values to search over\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [12, 16, 24, 32, 36, 48, 60, 64, 128],\n",
        "    'epochs': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'l2_regularization': [0.0001, 0.001, 0.01],\n",
        "    'filters': [16, 32, 64, 128, 256],\n",
        "    'kernel_size': [13, 15, 17, 19, 21],\n",
        "    'dropout': [0.3, 0.5, 0.7],\n",
        "    'activation': ['relu', 'sigmoid'],\n",
        "    'pool_size': [2, 3, 4],\n",
        "    'strides': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV instance to your data\n",
        "grid_result = grid_search.fit(train_x, train_y)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters: \", grid_result.best_params_)\n",
        "print(\"Best Score: \", grid_result.best_score_)\n"
      ]
    }
  ]
}